{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PNSR Criteria Calculation**"
      ],
      "metadata": {
        "id": "lqBYKEfm2wz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal in this code is to calculate the PNSR criteria for the generated dataset and the MPRNet pretrainted model as an example (calculation is the same for other networks we studied) to evaluate the performance of the networks. The PNSR calculation is done for random patches cropped from the image (with patch size \"PS\" chosen in the .YML file) similar to the calculation for MPRNet paper.\n",
        "\n",
        "Finally, the PSNR of the best saved model from training is also compared to other values, suggesting that the fine-tuning improved the performance sinificantly.\n",
        "\n",
        "**Note:** PNSR is not the optimization criterion when training the MPRNet, but it is the criteria that the training will decide to save the best model based on. PNSR is not differentiable, so Charbonnier loss is the main optimization criterion being used.\n",
        "\n",
        "##########################################################################################"
      ],
      "metadata": {
        "id": "wMzxmHx6059s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)** We are using the best configs of google colab to run all of our codes. Therefore, the first step is to mount google drive, where we have uploaded the MPRNet github codes + our datasets + pretrained and best trained models:"
      ],
      "metadata": {
        "id": "IfqI03AA3v6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyxJbMMYePSY",
        "outputId": "9fdbdf67-d6b3-4874-d582-06bbc47a9887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "#mount drive\n",
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)** Make the MPRNet directory, the main working directory:"
      ],
      "metadata": {
        "id": "-QFldM3b4Ksg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om5bhEIah-YI",
        "outputId": "2a2da726-9d8a-49d0-95d2-1eb20d4aab7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints   dataset_RGB.py\t     losses.py\t\t       training3.yml\n",
            "checkpoints2  Datasets\t\t     MPRNet.py\t\t       training4.yml\n",
            "CHK2\t      Datasets2\t\t     MPRNET_Training_V1.ipynb  training5.yml\n",
            "CHK3\t      Datasets3\t\t     pretrained_models\t       training6.yml\n",
            "CHK4\t      Datasets4\t\t     __pycache__\t       training7.yml\n",
            "CHK5\t      Datasets5\t\t     README.md\t\t       training_Nov.yml\n",
            "CHK6\t      evaluate_GOPRO_HIDE.m  Result\t\t       training.yml\n",
            "config.py     evaluate_RealBlur.py   test.py\t\t       train.py\n",
            "data_RGB.py   Input\t\t     training2.yml\t       utils\n"
          ]
        }
      ],
      "source": [
        "os.chdir('mydrive/MPRNET_V2/Deblurring')\n",
        "os.getcwd()\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3)** Install required packages:"
      ],
      "metadata": {
        "id": "PaKtIVB24hN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q09hm5U-jG-4",
        "outputId": "26cfe074-eef0-43ef-cf3b-ac8c5f3a1fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from yacs) (6.0)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install yacs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eN49YQojnVo",
        "outputId": "f8ea0814-7619-4f52-d54b-18e59af9b544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting warmup-scheduler\n",
            "  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2983 sha256=f5497449be9eeb0b1d0bc1e3e466c90db43eff9babdec6d3312339ac621c89a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/ce/4a/215c4f0add432420ff90fe04656bf2664ddfac7302e2b6fe51\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler\n",
            "Successfully installed warmup-scheduler-0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install warmup-scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4)** The .yml file includes the training hyperparameters (patch size, batch size, learning rate, etc.). Inaddition to those, the directories of the saved models and the datasets are given in the .yml file. We need to read this file here:"
      ],
      "metadata": {
        "id": "7LCiSojH5ZM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from config import Config \n",
        "opt = Config('training6.yml')\n",
        "\n",
        "gpus = ','.join([str(i) for i in opt.GPU])\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus"
      ],
      "metadata": {
        "id": "f8x5S2FW5XIP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5)** Import required libraries:"
      ],
      "metadata": {
        "id": "pXvlyHw_5NrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JV_smtAVVzr6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import utils\n",
        "from data_RGB import get_training_data, get_validation_data\n",
        "from MPRNet import MPRNet\n",
        "import losses\n",
        "from warmup_scheduler import GradualWarmupScheduler\n",
        "from tqdm import tqdm\n",
        "from pdb import set_trace as stx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6)** As mentioned, the PNSR calculation we use exploits random patches from the image. To make this calculation repetitive, we set the random seeds to fixed values to avoid changes each time we run the code:"
      ],
      "metadata": {
        "id": "hE7Jjqsr422f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5NwUa8GIjO6v"
      },
      "outputs": [],
      "source": [
        "######### Set Seeds ###########\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "start_epoch = 1\n",
        "mode = opt.MODEL.MODE\n",
        "session = opt.MODEL.SESSION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7)** Find the validation and training datasets directories based on the given \".yml\" file:"
      ],
      "metadata": {
        "id": "wsT4i47H50kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_dir = os.path.join(opt.TRAINING.SAVE_DIR, mode, 'results', session)\n",
        "model_dir  = os.path.join(opt.TRAINING.SAVE_DIR, mode, 'models',  session)\n",
        "\n",
        "utils.mkdir(result_dir)\n",
        "utils.mkdir(model_dir)\n",
        "\n",
        "train_dir = opt.TRAINING.TRAIN_DIR\n",
        "val_dir   = opt.TRAINING.VAL_DIR"
      ],
      "metadata": {
        "id": "XaB0o9yh5zep"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8)** Create dataloaders based on the dataset:"
      ],
      "metadata": {
        "id": "66vzx8qT7KRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######### DataLoaders ###########\n",
        "train_dataset = get_training_data(train_dir, {'patch_size':opt.TRAINING.TRAIN_PS})\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.OPTIM.BATCH_SIZE, shuffle=True, num_workers=16, drop_last=False, pin_memory=True)\n",
        "\n",
        "val_dataset = get_validation_data(val_dir, {'patch_size':opt.TRAINING.VAL_PS})\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=16, shuffle=False, num_workers=8, drop_last=False, pin_memory=True)\n",
        "\n",
        "print('===> Start Epoch {} End Epoch {}'.format(start_epoch,opt.OPTIM.NUM_EPOCHS + 1))\n",
        "print('===> Loading datasets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45JHNmP_6zUo",
        "outputId": "08540edd-9c87-4738-a603-699925e56392"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Start Epoch 1 End Epoch 3001\n",
            "===> Loading datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9)** This function gets the dataloader and defined model, and calculates the PNSR:"
      ],
      "metadata": {
        "id": "ECR62fc97Ptw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yuhbCw9IZPqq"
      },
      "outputs": [],
      "source": [
        "def get_pnsr(data_loader, model):\n",
        "    psnr_val_rgb = []\n",
        "    for ii, data_val in enumerate((data_loader), 0):\n",
        "        target = data_val[0].cuda()\n",
        "        input_ = data_val[1].cuda()\n",
        "        restored = input_\n",
        "\n",
        "        if model:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                restored = model(input_)\n",
        "            restored = restored[0]\n",
        "\n",
        "        for res,tar in zip(restored,target):\n",
        "            psnr_val_rgb.append(utils.torchPSNR(res, tar))\n",
        "\n",
        "    psnr_val_rgb  = torch.stack(psnr_val_rgb).mean().item()\n",
        "\n",
        "    return psnr_val_rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10)** This function finds the model in the \"name\" directory and calls the previous function:"
      ],
      "metadata": {
        "id": "UNvt6Uui7bXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MhAxiiBSeSVr"
      },
      "outputs": [],
      "source": [
        "def eval_mprnet(name, data_loader):\n",
        "  mprnet = MPRNet()\n",
        "  mprnet.cuda()\n",
        "  weights = utils.get_last_path(model_dir, name)\n",
        "\n",
        "  # utils.load_checkpoint(mprnet,weights)\n",
        "  # mprnet.load_state_dict(torch.load)\n",
        "\n",
        "  # mprnet.load_state_dict(torch.load)\n",
        "\n",
        "  checkpoint = torch.load(weights)\n",
        "  mprnet.load_state_dict(checkpoint[\"state_dict\"])\n",
        "  return get_pnsr(data_loader, mprnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11)** Results:"
      ],
      "metadata": {
        "id": "27C_Wn9s7l7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to improve the quality of test dataset images by deblurring. The PNSR of test dataset is what we need to increase. It is important to have some sense about the PNSR of the dataset we are trying to deblur:\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "U4AyHYhQ7oV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cslCTfmpboSa",
        "outputId": "30d6db24-961c-481f-f1c8-a5c9019e0ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Dataset PNSR: 26.814794540405273\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Dataset PNSR:\", get_pnsr(val_loader, None))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to know how the ***pretrained model*** (our initial model) works on the dataset:"
      ],
      "metadata": {
        "id": "b6hZAcoQ8EWS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3jQFIIgfQPU",
        "outputId": "ddb5e7d3-bb4c-4d39-9ab8-51628df12704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained Model PNSR Over Test Dataset: 27.67173194885254\n"
          ]
        }
      ],
      "source": [
        "print(\"Pretrained Model PNSR Over Test Dataset:\", eval_mprnet(\"_pretrained.pth\", val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of the ***fine-tuned model*** on the dataset:"
      ],
      "metadata": {
        "id": "s5s-m38c8ew-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fine-Tuned Model PNSR Over Test Dataset:\", eval_mprnet(\"_best.pth\", val_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EIXnauUq34W",
        "outputId": "4f6e1053-076f-476e-d785-2dd823f79fe5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Model PNSR Over Test Dataset: 31.992860794067383\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}